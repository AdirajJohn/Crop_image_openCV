{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3fJfOb9-D2z"
      },
      "source": [
        "### Custom Object Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QjP6WbnnuFD",
        "outputId": "7eaa7d94-5384-4afb-f845-2de5a0e6a67d"
      },
      "outputs": [],
      "source": [
        "#!pip install ultralytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y0O9n3fxNBJ"
      },
      "source": [
        "### Import Necessary Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8V4Y1RU2xSPw",
        "outputId": "79fd6ea1-7751-4f76-8c85-aba484e061f8"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "from IPython.display import Image,display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBklIjUujBDy"
      },
      "source": [
        "### Load the pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOuzlAJkjF1S",
        "outputId": "eba887b5-2152-47d8-dd40-34bc1f43fce8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
            "YOLOv8n summary: 225 layers, 3157200 parameters, 3157184 gradients\n",
            "\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n",
            "100%|██████████| 6.23M/6.23M [00:08<00:00, 766kB/s] \n",
            "Transferred 355/355 items from pretrained weights\n"
          ]
        }
      ],
      "source": [
        "model = YOLO('yolov8n.yaml').load('yolov8n.pt')  # build from YAML and transfer weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COqn3TFy4j10"
      },
      "source": [
        "### Train the model on the custom dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyt_cgGDyffF",
        "outputId": "e4b7ec9b-489e-41da-c9d4-91ba82ce1593"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "New https://pypi.org/project/ultralytics/8.0.181 available  Update with 'pip install -U ultralytics'\n",
            "Ultralytics YOLOv8.0.180  Python-3.10.10 torch-2.0.1+cpu CPU (Intel Core(TM) i5-8250U 1.60GHz)\n",
            "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.yaml, data=data.yaml, epochs=20, patience=50, batch=16, imgsz=232, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train4\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
            "YOLOv8n summary: 225 layers, 3011043 parameters, 3011027 gradients\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train4', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "WARNING  imgsz=[232] must be multiple of max stride 32, updating to [256]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\repository\\YOLOv8_ObjectDetection\\YOLOv8\\data\\train\\labels... 82 images, 1 backgrounds, 0 corrupt: 100%|██████████| 83/83 [00:00<00:00, 589.23it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: D:\\repository\\YOLOv8_ObjectDetection\\YOLOv8\\data\\train\\labels.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\repository\\YOLOv8_ObjectDetection\\YOLOv8\\data\\valid\\labels... 16 images, 0 backgrounds, 0 corrupt: 100%|██████████| 16/16 [00:00<00:00, 751.05it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: D:\\repository\\YOLOv8_ObjectDetection\\YOLOv8\\data\\valid\\labels.cache\n",
            "Plotting labels to runs\\detect\\train4\\labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 256 train, 256 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns\\detect\\train4\u001b[0m\n",
            "Starting training for 20 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/20         0G      2.045      3.254      1.809          4        256: 100%|██████████| 6/6 [00:10<00:00,  1.68s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
            "                   all         16         16    0.00932          1      0.995      0.151\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       2/20         0G      1.518      2.718      1.352          5        256: 100%|██████████| 6/6 [00:09<00:00,  1.64s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.22it/s]\n",
            "                   all         16         16    0.00955          1      0.925      0.582\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       3/20         0G      1.213      1.519      1.131          4        256: 100%|██████████| 6/6 [00:11<00:00,  1.97s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n",
            "                   all         16         16      0.981          1      0.995      0.818\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       4/20         0G     0.9685      1.105      1.071          5        256: 100%|██████████| 6/6 [00:10<00:00,  1.82s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.25it/s]\n",
            "                   all         16         16      0.993          1      0.995      0.559\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       5/20         0G     0.9477     0.9974      1.038         10        256: 100%|██████████| 6/6 [00:11<00:00,  1.87s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n",
            "                   all         16         16    0.00794          1      0.995      0.579\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       6/20         0G     0.9452      1.029      1.041          7        256: 100%|██████████| 6/6 [00:10<00:00,  1.79s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.21it/s]\n",
            "                   all         16         16      0.985          1      0.995      0.642\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       7/20         0G     0.8547     0.8287      1.014          8        256: 100%|██████████| 6/6 [00:10<00:00,  1.71s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.30it/s]\n",
            "                   all         16         16      0.996          1      0.995      0.826\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       8/20         0G     0.7741     0.7897     0.9861          5        256: 100%|██████████| 6/6 [00:11<00:00,  1.83s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.35it/s]\n",
            "                   all         16         16      0.991          1      0.995      0.658\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       9/20         0G      1.029     0.8081      1.041          6        256: 100%|██████████| 6/6 [00:09<00:00,  1.66s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n",
            "                   all         16         16      0.993          1      0.995      0.747\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      10/20         0G     0.9017     0.7969     0.9994          7        256: 100%|██████████| 6/6 [00:10<00:00,  1.69s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.26it/s]\n",
            "                   all         16         16      0.996          1      0.995      0.855\n",
            "Closing dataloader mosaic\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      11/20         0G     0.8069     0.9421      0.976          3        256: 100%|██████████| 6/6 [00:09<00:00,  1.66s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.31it/s]\n",
            "                   all         16         16      0.997          1      0.995       0.81\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      12/20         0G     0.6934      0.879     0.9382          3        256: 100%|██████████| 6/6 [00:09<00:00,  1.63s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.30it/s]\n",
            "                   all         16         16      0.996          1      0.995      0.771\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      13/20         0G      0.553     0.7214     0.9167          3        256: 100%|██████████| 6/6 [00:09<00:00,  1.63s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.07it/s]\n",
            "                   all         16         16      0.997          1      0.995      0.811\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      14/20         0G     0.5139     0.6375     0.8952          3        256: 100%|██████████| 6/6 [00:10<00:00,  1.71s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.31it/s]\n",
            "                   all         16         16      0.997          1      0.995       0.86\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      15/20         0G     0.4766     0.6215     0.8955          3        256: 100%|██████████| 6/6 [00:09<00:00,  1.66s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.26it/s]\n",
            "                   all         16         16      0.997          1      0.995      0.897\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      16/20         0G     0.5136     0.5753       0.87          3        256: 100%|██████████| 6/6 [00:10<00:00,  1.69s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.27it/s]\n",
            "                   all         16         16      0.997          1      0.995      0.913\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      17/20         0G     0.4558     0.5503       0.87          3        256: 100%|██████████| 6/6 [00:09<00:00,  1.62s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.27it/s]\n",
            "                   all         16         16      0.997          1      0.995      0.965\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      18/20         0G     0.4556     0.5647     0.8462          3        256: 100%|██████████| 6/6 [00:09<00:00,  1.67s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n",
            "                   all         16         16      0.997          1      0.995      0.995\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      19/20         0G     0.4593     0.5594     0.9175          3        256: 100%|██████████| 6/6 [00:10<00:00,  1.68s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.26it/s]\n",
            "                   all         16         16      0.997          1      0.995      0.969\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      20/20         0G     0.4495      0.597     0.9039          3        256: 100%|██████████| 6/6 [00:09<00:00,  1.66s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.31it/s]\n",
            "                   all         16         16      0.997          1      0.995      0.941\n",
            "\n",
            "20 epochs completed in 0.067 hours.\n",
            "Optimizer stripped from runs\\detect\\train4\\weights\\last.pt, 6.2MB\n",
            "Optimizer stripped from runs\\detect\\train4\\weights\\best.pt, 6.2MB\n",
            "\n",
            "Validating runs\\detect\\train4\\weights\\best.pt...\n",
            "Ultralytics YOLOv8.0.180  Python-3.10.10 torch-2.0.1+cpu CPU (Intel Core(TM) i5-8250U 1.60GHz)\n",
            "YOLOv8n summary (fused): 168 layers, 3005843 parameters, 0 gradients\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.63it/s]\n",
            "                   all         16         16      0.997          1      0.995      0.995\n",
            "Speed: 0.3ms preprocess, 23.0ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
            "Results saved to \u001b[1mruns\\detect\\train4\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "results = model.train(data='data.yaml', epochs=20, imgsz=232)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cspEs9dE84cf"
      },
      "source": [
        "### Model Validation on custom dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NaeDg3u5HUq",
        "outputId": "ca808ae6-aabb-4dd7-bd23-2dc5d9477f06"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.0.180  Python-3.10.10 torch-2.0.1+cpu CPU (Intel Core(TM) i5-8250U 1.60GHz)\n",
            "YOLOv8n summary (fused): 168 layers, 3005843 parameters, 0 gradients\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\repository\\YOLOv8_ObjectDetection\\YOLOv8\\data\\valid\\labels.cache... 16 images, 0 backgrounds, 0 corrupt: 100%|██████████| 16/16 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.65it/s]\n",
            "                   all         16         16      0.997          1      0.995      0.995\n",
            "Speed: 0.4ms preprocess, 24.1ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
            "Results saved to \u001b[1mruns\\detect\\val\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#Load the trained model\n",
        "model=YOLO('D:/repository/YOLOv8_ObjectDetection/runs/detect/train4/weights/best.pt')  # load a custom model\n",
        "# Validate the model\n",
        "metrics = model.val()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5Pflhsp-2iC"
      },
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3xSLMLu93yd",
        "outputId": "1e8c9206-e616-44fd-f32c-5396be5b90e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "image 1/1 D:\\repository\\YOLOv8_ObjectDetection\\YOLOv8\\data\\test\\images\\W27-1005_20230815_094740.png: 160x256 1 crop, 58.1ms\n",
            "Speed: 1.0ms preprocess, 58.1ms inference, 1.0ms postprocess per image at shape (1, 3, 160, 256)\n"
          ]
        }
      ],
      "source": [
        "pred=model(\"D:/repository/YOLOv8_ObjectDetection/YOLOv8/data/test/images/W27-1005_20230815_094740.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_JfiHYZFM61"
      },
      "source": [
        "### Predict and Save the bounding box"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o9M-muSAiPO",
        "outputId": "4dcf3d45-6ae2-44c2-f4c7-754adaa9b37a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING  imgsz=[232] must be multiple of max stride 32, updating to [256]\n",
            "image 1/1 D:\\repository\\YOLOv8_ObjectDetection\\YOLOv8\\data\\test\\images\\W27-1005_20230815_094740.png: 160x256 1 crop, 49.9ms\n",
            "Speed: 1.0ms preprocess, 49.9ms inference, 1.0ms postprocess per image at shape (1, 3, 160, 256)\n",
            "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[ultralytics.engine.results.Results object with attributes:\n",
              " \n",
              " boxes: ultralytics.engine.results.Boxes object\n",
              " keypoints: None\n",
              " masks: None\n",
              " names: {0: 'crop'}\n",
              " orig_img: array([[[255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         ...,\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255]],\n",
              " \n",
              "        [[255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         ...,\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255]],\n",
              " \n",
              "        [[255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         ...,\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         ...,\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255]],\n",
              " \n",
              "        [[255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         ...,\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255]],\n",
              " \n",
              "        [[255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         ...,\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255]]], dtype=uint8)\n",
              " orig_shape: (600, 973)\n",
              " path: 'D:\\\\repository\\\\YOLOv8_ObjectDetection\\\\YOLOv8\\\\data\\\\test\\\\images\\\\W27-1005_20230815_094740.png'\n",
              " probs: None\n",
              " save_dir: None\n",
              " speed: {'preprocess': 0.9968280792236328, 'inference': 49.85857009887695, 'postprocess': 0.9977817535400391}]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.predict(\"D:/repository/YOLOv8_ObjectDetection/YOLOv8/data/test/images/W27-1005_20230815_094740.png\", save_crop=True, imgsz=232, conf=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hixiD9ZO_JyR",
        "outputId": "51e9c396-b94b-48b2-8979-eab7b24477d1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING ⚠️ imgsz=[232] must be multiple of max stride 32, updating to [256]\n",
            "image 1/1 /content/drive/MyDrive/YOLOv8/data/test/images/W28-1001_20230815_094818.png: 160x256 1 crop, 10.3ms\n",
            "Speed: 1.2ms preprocess, 10.3ms inference, 2.0ms postprocess per image at shape (1, 3, 160, 256)\n",
            "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[ultralytics.engine.results.Results object with attributes:\n",
              " \n",
              " boxes: ultralytics.engine.results.Boxes object\n",
              " keypoints: None\n",
              " masks: None\n",
              " names: {0: 'crop'}\n",
              " orig_img: array([[[255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         ...,\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255]],\n",
              " \n",
              "        [[255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         ...,\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255]],\n",
              " \n",
              "        [[255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         ...,\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         ...,\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255]],\n",
              " \n",
              "        [[255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         ...,\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255]],\n",
              " \n",
              "        [[255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         ...,\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255]]], dtype=uint8)\n",
              " orig_shape: (600, 973)\n",
              " path: '/content/drive/MyDrive/YOLOv8/data/test/images/W28-1001_20230815_094818.png'\n",
              " probs: None\n",
              " save_dir: None\n",
              " speed: {'preprocess': 1.1699199676513672, 'inference': 10.335683822631836, 'postprocess': 2.0301342010498047}]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.predict(\"/content/drive/MyDrive/YOLOv8/data/test/images/W28-1001_20230815_094818.png\", save_crop=True, imgsz=232, conf=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG9F4IbPRtHb"
      },
      "source": [
        "### Predict and crop the all the image in the folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "veUeke8A_QGZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24\n"
          ]
        }
      ],
      "source": [
        "folder_path=\"D:/repository/YOLOv8_ObjectDetection/TO_Crop\"\n",
        "#List of all the file in a folder\n",
        "file_name=os.listdir(folder_path)\n",
        "#To print the count of the image in a folder\n",
        "print(len(file_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Destination Folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_folder_path=\"D:/repository/YOLOv8_ObjectDetection/After_crop\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "image 1/1 D:\\repository\\YOLOv8_ObjectDetection\\TO_Crop\\W24-1003_20230815_093412.png: 160x256 1 crop, 52.2ms\n",
            "Speed: 1.0ms preprocess, 52.2ms inference, 2.1ms postprocess per image at shape (1, 3, 160, 256)\n",
            "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "D:/repository/YOLOv8_ObjectDetection/TO_Crop/W24-1003_20230815_093412.png\n",
            "D:/repository/YOLOv8_ObjectDetection/TO_Crop/W24-1004_20230815_093416.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "image 1/1 D:\\repository\\YOLOv8_ObjectDetection\\TO_Crop\\W24-1004_20230815_093416.png: 160x256 1 crop, 90.7ms\n",
            "Speed: 1.0ms preprocess, 90.7ms inference, 1.0ms postprocess per image at shape (1, 3, 160, 256)\n",
            "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n",
            "\n",
            "image 1/1 D:\\repository\\YOLOv8_ObjectDetection\\TO_Crop\\W24-1005_20230815_093422.png: 160x256 1 crop, 37.7ms\n",
            "Speed: 1.0ms preprocess, 37.7ms inference, 1.0ms postprocess per image at shape (1, 3, 160, 256)\n",
            "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n",
            "\n",
            "image 1/1 D:\\repository\\YOLOv8_ObjectDetection\\TO_Crop\\W24-1006_20230815_093426.png: 160x256 1 crop, 41.8ms\n",
            "Speed: 1.0ms preprocess, 41.8ms inference, 2.0ms postprocess per image at shape (1, 3, 160, 256)\n",
            "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "D:/repository/YOLOv8_ObjectDetection/TO_Crop/W24-1005_20230815_093422.png\n",
            "D:/repository/YOLOv8_ObjectDetection/TO_Crop/W24-1006_20230815_093426.png\n",
            "D:/repository/YOLOv8_ObjectDetection/TO_Crop/W25-1001_20230815_093434.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "image 1/1 D:\\repository\\YOLOv8_ObjectDetection\\TO_Crop\\W25-1001_20230815_093434.png: 160x256 1 crop, 37.6ms\n",
            "Speed: 0.0ms preprocess, 37.6ms inference, 3.5ms postprocess per image at shape (1, 3, 160, 256)\n",
            "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n",
            "\n",
            "image 1/1 D:\\repository\\YOLOv8_ObjectDetection\\TO_Crop\\W25-1002_20230815_093443.png: 160x256 1 crop, 76.9ms\n",
            "Speed: 1.0ms preprocess, 76.9ms inference, 2.0ms postprocess per image at shape (1, 3, 160, 256)\n",
            "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "D:/repository/YOLOv8_ObjectDetection/TO_Crop/W25-1002_20230815_093443.png\n",
            "D:/repository/YOLOv8_ObjectDetection/TO_Crop/W25-1003_20230815_093449.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "image 1/1 D:\\repository\\YOLOv8_ObjectDetection\\TO_Crop\\W25-1003_20230815_093449.png: 160x256 1 crop, 100.7ms\n",
            "Speed: 1.0ms preprocess, 100.7ms inference, 0.8ms postprocess per image at shape (1, 3, 160, 256)\n",
            "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n",
            "\n",
            "image 1/1 D:\\repository\\YOLOv8_ObjectDetection\\TO_Crop\\W25-1004_20230815_093455.png: 160x256 1 crop, 57.3ms\n",
            "Speed: 0.7ms preprocess, 57.3ms inference, 2.1ms postprocess per image at shape (1, 3, 160, 256)\n",
            "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n",
            "\n",
            "image 1/1 D:\\repository\\YOLOv8_ObjectDetection\\TO_Crop\\W25-1005_20230815_093501.png: 160x256 1 crop, 43.0ms\n",
            "Speed: 1.0ms preprocess, 43.0ms inference, 1.0ms postprocess per image at shape (1, 3, 160, 256)\n",
            "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "D:/repository/YOLOv8_ObjectDetection/TO_Crop/W25-1004_20230815_093455.png\n",
            "D:/repository/YOLOv8_ObjectDetection/TO_Crop/W25-1005_20230815_093501.png\n",
            "D:/repository/YOLOv8_ObjectDetection/TO_Crop/W25-1006_20230815_093515.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "image 1/1 D:\\repository\\YOLOv8_ObjectDetection\\TO_Crop\\W25-1006_20230815_093515.png: 160x256 1 crop, 31.4ms\n",
            "Speed: 1.0ms preprocess, 31.4ms inference, 1.4ms postprocess per image at shape (1, 3, 160, 256)\n",
            "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n",
            "\n",
            "image 1/1 D:\\repository\\YOLOv8_ObjectDetection\\TO_Crop\\W26-1001_20230815_093522.png: 160x256 1 crop, 44.4ms\n",
            "Speed: 0.2ms preprocess, 44.4ms inference, 1.0ms postprocess per image at shape (1, 3, 160, 256)\n",
            "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n",
            "\n",
            "image 1/1 D:\\repository\\YOLOv8_ObjectDetection\\TO_Crop\\W3-1001_20230815_100932.png: 160x256 1 crop, 39.6ms\n",
            "Speed: 1.2ms preprocess, 39.6ms inference, 2.0ms postprocess per image at shape (1, 3, 160, 256)\n",
            "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n",
            "\n",
            "image 1/1 D:\\repository\\YOLOv8_ObjectDetection\\TO_Crop\\W3-1002_20230815_100939.png: 160x256 1 crop, 47.4ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "D:/repository/YOLOv8_ObjectDetection/TO_Crop/W26-1001_20230815_093522.png\n",
            "D:/repository/YOLOv8_ObjectDetection/TO_Crop/W3-1001_20230815_100932.png\n",
            "D:/repository/YOLOv8_ObjectDetection/TO_Crop/W3-1002_20230815_100939.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Speed: 1.0ms preprocess, 47.4ms inference, 1.0ms postprocess per image at shape (1, 3, 160, 256)\n",
            "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n",
            "\n",
            "image 1/1 D:\\repository\\YOLOv8_ObjectDetection\\TO_Crop\\W3-1003_20230815_100943.png: 160x256 1 crop, 84.8ms\n",
            "Speed: 0.6ms preprocess, 84.8ms inference, 1.9ms postprocess per image at shape (1, 3, 160, 256)\n",
            "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "D:/repository/YOLOv8_ObjectDetection/TO_Crop/W3-1003_20230815_100943.png\n",
            "D:/repository/YOLOv8_ObjectDetection/TO_Crop/W3-1004_20230815_100946.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "image 1/1 D:\\repository\\YOLOv8_ObjectDetection\\TO_Crop\\W3-1004_20230815_100946.png: 160x256 1 crop, 65.0ms\n",
            "Speed: 1.0ms preprocess, 65.0ms inference, 1.9ms postprocess per image at shape (1, 3, 160, 256)\n",
            "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n",
            "\n",
            "image 1/1 D:\\repository\\YOLOv8_ObjectDetection\\TO_Crop\\W3-1005_20230815_100949.png: 160x256 1 crop, 63.5ms\n",
            "Speed: 1.0ms preprocess, 63.5ms inference, 1.0ms postprocess per image at shape (1, 3, 160, 256)\n",
            "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n",
            "\n",
            "image 1/1 D:\\repository\\YOLOv8_ObjectDetection\\TO_Crop\\W3-1006_20230815_100954.png: 160x256 1 crop, 46.9ms\n",
            "Speed: 1.0ms preprocess, 46.9ms inference, 1.1ms postprocess per image at shape (1, 3, 160, 256)\n",
            "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "D:/repository/YOLOv8_ObjectDetection/TO_Crop/W3-1005_20230815_100949.png\n",
            "D:/repository/YOLOv8_ObjectDetection/TO_Crop/W3-1006_20230815_100954.png\n",
            "D:/repository/YOLOv8_ObjectDetection/TO_Crop/W4-1002_20230815_102742.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "image 1/1 D:\\repository\\YOLOv8_ObjectDetection\\TO_Crop\\W4-1002_20230815_102742.png: 160x256 1 crop, 68.2ms\n",
            "Speed: 1.0ms preprocess, 68.2ms inference, 2.0ms postprocess per image at shape (1, 3, 160, 256)\n",
            "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "D:/repository/YOLOv8_ObjectDetection/TO_Crop/W4-1003_20230815_102752.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "image 1/1 D:\\repository\\YOLOv8_ObjectDetection\\TO_Crop\\W4-1003_20230815_102752.png: 160x256 1 crop, 182.4ms\n",
            "Speed: 0.5ms preprocess, 182.4ms inference, 1.0ms postprocess per image at shape (1, 3, 160, 256)\n",
            "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n",
            "\n",
            "image 1/1 D:\\repository\\YOLOv8_ObjectDetection\\TO_Crop\\W4-1004_20230815_102803.png: 160x256 1 crop, 88.7ms\n",
            "Speed: 2.0ms preprocess, 88.7ms inference, 3.8ms postprocess per image at shape (1, 3, 160, 256)\n",
            "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "D:/repository/YOLOv8_ObjectDetection/TO_Crop/W4-1004_20230815_102803.png\n",
            "D:/repository/YOLOv8_ObjectDetection/TO_Crop/W4-1005_20230815_102816.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "image 1/1 D:\\repository\\YOLOv8_ObjectDetection\\TO_Crop\\W4-1005_20230815_102816.png: 160x256 1 crop, 92.5ms\n",
            "Speed: 1.0ms preprocess, 92.5ms inference, 2.0ms postprocess per image at shape (1, 3, 160, 256)\n",
            "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n",
            "\n",
            "image 1/1 D:\\repository\\YOLOv8_ObjectDetection\\TO_Crop\\W4-1006_20230815_102832.png: 160x256 1 crop, 59.0ms\n",
            "Speed: 1.0ms preprocess, 59.0ms inference, 1.0ms postprocess per image at shape (1, 3, 160, 256)\n",
            "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "D:/repository/YOLOv8_ObjectDetection/TO_Crop/W4-1006_20230815_102832.png\n",
            "D:/repository/YOLOv8_ObjectDetection/TO_Crop/W4-2001_20230815_102845.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "image 1/1 D:\\repository\\YOLOv8_ObjectDetection\\TO_Crop\\W4-2001_20230815_102845.png: 160x256 1 crop, 77.4ms\n",
            "Speed: 1.5ms preprocess, 77.4ms inference, 2.0ms postprocess per image at shape (1, 3, 160, 256)\n",
            "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n",
            "\n",
            "image 1/1 D:\\repository\\YOLOv8_ObjectDetection\\TO_Crop\\W4-2002_20230815_102858.png: 160x256 1 crop, 72.0ms\n",
            "Speed: 1.1ms preprocess, 72.0ms inference, 1.0ms postprocess per image at shape (1, 3, 160, 256)\n",
            "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "D:/repository/YOLOv8_ObjectDetection/TO_Crop/W4-2002_20230815_102858.png\n"
          ]
        }
      ],
      "source": [
        "#PREDICT AND CROP\n",
        "for i in range(len(file_name)):\n",
        "    src=\"D:/repository/YOLOv8_ObjectDetection/TO_Crop/\"+str(file_name[i])\n",
        "    model.predict(src,save_crop=True,imgsz=256, conf=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
